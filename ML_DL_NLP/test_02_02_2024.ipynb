{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What you understand by Text Processing? Write a code to perform text processing"
      ],
      "metadata": {
        "id": "36P61SpTmPvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text processing in Natural Language Processing (NLP) involves manipulating and analyzing textual data to extract useful information.\n",
        "# It encompasses various tasks such as tokenization, stemming, lemmatization etc.,\n",
        "# Tokenization: Breaking text into smaller units, usually words or sentences.\n",
        "# Normalization: Standardizing text by converting it to lowercase, removing punctuation, etc.\n",
        "# Stopword Removal: Filtering out common words that don't carry much meaning.\n",
        "# Stemming and Lemmatization: Reducing words to their base or root form.\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "text = '''Machine learning, deep learning, and artificial intelligence are subsets of data science,\n",
        " with each focusing on specific techniques and methodologies for analyzing and utilizing data.\"Data science is a subset of machine learning,\n",
        " deep learning and artificial intelligence.'''\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print('word_tokenizer: ',tokens)\n",
        "# Normalization\n",
        "normalized_word = [word.lower() for word in tokens if word.isalnum()]\n",
        "print('normalized_word: ',normalized_word)\n",
        "# Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in normalized_word if word not in stop_words]\n",
        "print('stopwords_removal: ',filtered_tokens)\n",
        "# Stemming\n",
        "ps = PorterStemmer()\n",
        "stemmed_tokens = [ps.stem(word) for word in filtered_tokens]\n",
        "print('stemming: ',stemmed_tokens)\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print('lemmatization: ',lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTy8vKVVtaLn",
        "outputId": "026f73b5-5d4b-4da2-bc9b-7c910bdb86c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_tokenizer:  ['Machine', 'learning', ',', 'deep', 'learning', ',', 'and', 'artificial', 'intelligence', 'are', 'subsets', 'of', 'data', 'science', ',', 'with', 'each', 'focusing', 'on', 'specific', 'techniques', 'and', 'methodologies', 'for', 'analyzing', 'and', 'utilizing', 'data', '.', '``', 'Data', 'science', 'is', 'a', 'subset', 'of', 'machine', 'learning', ',', 'deep', 'learning', 'and', 'artificial', 'intelligence', '.']\n",
            "normalized_word:  ['machine', 'learning', 'deep', 'learning', 'and', 'artificial', 'intelligence', 'are', 'subsets', 'of', 'data', 'science', 'with', 'each', 'focusing', 'on', 'specific', 'techniques', 'and', 'methodologies', 'for', 'analyzing', 'and', 'utilizing', 'data', 'data', 'science', 'is', 'a', 'subset', 'of', 'machine', 'learning', 'deep', 'learning', 'and', 'artificial', 'intelligence']\n",
            "stopwords_removal:  ['machine', 'learning', 'deep', 'learning', 'artificial', 'intelligence', 'subsets', 'data', 'science', 'focusing', 'specific', 'techniques', 'methodologies', 'analyzing', 'utilizing', 'data', 'data', 'science', 'subset', 'machine', 'learning', 'deep', 'learning', 'artificial', 'intelligence']\n",
            "stemming:  ['machin', 'learn', 'deep', 'learn', 'artifici', 'intellig', 'subset', 'data', 'scienc', 'focus', 'specif', 'techniqu', 'methodolog', 'analyz', 'util', 'data', 'data', 'scienc', 'subset', 'machin', 'learn', 'deep', 'learn', 'artifici', 'intellig']\n",
            "lemmatization:  ['machine', 'learning', 'deep', 'learning', 'artificial', 'intelligence', 'subset', 'data', 'science', 'focusing', 'specific', 'technique', 'methodology', 'analyzing', 'utilizing', 'data', 'data', 'science', 'subset', 'machine', 'learning', 'deep', 'learning', 'artificial', 'intelligence']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What you understand by NLP toolkit and spacy library? Write a code in which any one gets used."
      ],
      "metadata": {
        "id": "tmr9dsaemU9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Natural Language Processing (NLP) toolkits and libraries provide developers with pre-built tools and functionalities to process and analyze\n",
        "# text data efficiently.\n",
        "# These tools typically include functions for tasks such as tokenization, POS tagging etc.,\n",
        "\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = '''Machine learning, deep learning, and artificial intelligence are subsets of data science,\n",
        " with each focusing on specific techniques and methodologies for analyzing and utilizing data.\"Data science is a subset of machine learning,\n",
        "deep learning and artificial intelligence.'''\n",
        "# NLTK tokenization\n",
        "nltk_tokens = nltk.word_tokenize(text)\n",
        "print(\"NLTK Tokens:\", nltk_tokens)\n",
        "# spaCy tokenization\n",
        "spacy_doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in spacy_doc]\n",
        "print(\"spaCy Tokens:\", spacy_tokens)"
      ],
      "metadata": {
        "id": "oniJOOinyhe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Describe Neural Networks and Deep Learning in Depth"
      ],
      "metadata": {
        "id": "cMTRInrNmU-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural networks are a class of machine learning models inspired by the structure and function of the human brain.\n",
        "# They consist of interconnected nodes organized into layers. Each node, also known as a neuron, receives input signals, processes them, and\n",
        "# produces an output signal. Neural networks learn to perform tasks by adjusting the weights associated with connections between neurons.\n",
        "# There are three layers such as input layer, hidden layer and output layer.\n",
        "\n",
        "# Deep learning is a subset of machine learning that uses neural networks with multiple hidden layers (hence the term \"deep\").\n",
        "# Deep learning algorithms automatically learn hierarchical representations of data, enabling them to capture intricate patterns\n",
        "# and features from raw input."
      ],
      "metadata": {
        "id": "MJFhLfO10aOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. what you understand by Hyperparameter Tuning?"
      ],
      "metadata": {
        "id": "3-xLK_9wnRrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning involves the process of selecting the optimal configuration for hyperparameters, which are parameters that govern the\n",
        "# training process of machine learning models. It aims to improve a model's performance by systematically searching through different hyperparameter\n",
        "# combinations, typically using techniques like grid search, random search, or Bayesian optimization. The goal is to find the set of hyperparameters\n",
        "# that maximizes the model's accuracy, generalization, or other performance metrics on a validation dataset."
      ],
      "metadata": {
        "id": "PpaAlPJa0vR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What you understand by Ensemble Learning?"
      ],
      "metadata": {
        "id": "MoAM0McCnRca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble learning is a machine learning technique where multiple models are combined to improve overall performance.\n",
        "# It works by aggregating the predictions of multiple base models, often using methods like averaging, voting, or stacking.\n",
        "#  Ensemble methods aim to reduce variance, increase robustness, and enhance predictive accuracy by leveraging the diversity of different models\n",
        "# and their individual strengths. Common ensemble learning algorithms include Random Forest, Gradient Boosting, and AdaBoost."
      ],
      "metadata": {
        "id": "qeNKj17y1TW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "6. What do you understand by Model Evaluation and Selection ?\n"
      ],
      "metadata": {
        "id": "gG9gehJKmVJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation and selection is the process of assessing the performance of machine learning models and\n",
        "# choosing the most suitable one for a given task or dataset.\n",
        "# It involves several steps such as evaluation metrics, model comparison, hyperparameter tuning."
      ],
      "metadata": {
        "id": "zVLT_2GV1p-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What you understand by Feature Engineering and Feature selection? What is the difference between them?"
      ],
      "metadata": {
        "id": "Cu9HmHxZm_4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering is the process of creating new features or transforming existing ones from raw data to improve\n",
        "# the performance of machine learning models. Feature engineering often includes techniques such as encoding categorical variables,\n",
        "# scaling numerical features, handling missing values.\n",
        "\n",
        "# Feature selection, on the other hand, is the process of selecting a subset of relevant features from the original set of features to\n",
        "# improve model performance, reduce dimensionality.\n",
        "\n",
        "# Feature engineering focuses on creating and transforming features to make them more informative and suitable for modeling,\n",
        "# while feature selection focuses on identifying and retaining the most relevant features to improve model performance."
      ],
      "metadata": {
        "id": "HZcsCk0q2eLT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}