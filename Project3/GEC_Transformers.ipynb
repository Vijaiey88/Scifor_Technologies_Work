{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "hptGjDCL9PQp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSZ1Lo3H9T-C",
        "outputId": "fbaa57de-6b3f-408d-8929-b0b3d17882df"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from CSV file\n",
        "data = pd.read_csv('/content/Grammar Correction.csv')\n",
        "# data = data.head(500)\n",
        "# Extract erroneous and corrected sentences\n",
        "erroneous_sentences = data['Ungrammatical Statement'].tolist()\n",
        "corrected_sentences = data['Standard English'].tolist()"
      ],
      "metadata": {
        "id": "gLUcgM8I9YMn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pad sequences to the same length\n",
        "def pad_sequences(sequences, max_length, pad_value=0):\n",
        "    padded_sequences = []\n",
        "    for seq in sequences:\n",
        "        if len(seq) < max_length:\n",
        "            padded_seq = seq + [pad_value] * (max_length - len(seq))\n",
        "        else:\n",
        "            padded_seq = seq[:max_length]\n",
        "        padded_sequences.append(padded_seq)\n",
        "    return padded_sequences"
      ],
      "metadata": {
        "id": "4dHO2Tlr-vY6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "HRKtwE7ZZYEi"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize input sentences and prepare input tensors\n",
        "tokenized_inputs = tokenizer(erroneous_sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "labels = tokenizer(corrected_sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Convert token IDs to tensors\n",
        "input_ids = tokenized_inputs['input_ids']\n",
        "attention_mask = tokenized_inputs['attention_mask']\n",
        "labels_ids = labels['input_ids']\n",
        "\n",
        "# Pad sequences to the same length\n",
        "max_length = max(len(seq) for seq in input_ids)\n",
        "input_ids = pad_sequences(input_ids.tolist(), max_length)\n",
        "attention_mask = pad_sequences(attention_mask.tolist(), max_length)\n",
        "labels_ids = pad_sequences(labels_ids.tolist(), max_length)\n",
        "\n",
        "# Convert padded sequences to tensors\n",
        "input_ids = torch.tensor(input_ids).to(device)\n",
        "attention_mask = torch.tensor(attention_mask).to(device)\n",
        "labels_ids = torch.tensor(labels_ids).to(device)"
      ],
      "metadata": {
        "id": "4ysb2MST9aoA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare DataLoader\n",
        "dataset = TensorDataset(input_ids, attention_mask, labels_ids)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "ra7vuNdT9cG4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune BERT model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "model.to(device)\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}'):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)  # Move tensors to GPU\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {total_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8AZE-Hc9N17",
        "outputId": "facf551b-ccce-45b9-fc59-90bd67792c43"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 100/100 [00:13<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 242.8673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 100/100 [00:13<00:00,  7.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 77.5569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 100/100 [00:13<00:00,  7.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 57.1861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 100/100 [00:13<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 43.2066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 100/100 [00:13<00:00,  7.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 32.0104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 100/100 [00:13<00:00,  7.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Loss: 23.9734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 100/100 [00:13<00:00,  7.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Loss: 18.1475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 100/100 [00:13<00:00,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Loss: 13.4467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 100/100 [00:13<00:00,  7.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Loss: 9.8241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 100/100 [00:13<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 7.2337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 100/100 [00:13<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Loss: 5.4784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 100/100 [00:13<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Loss: 4.2750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 100/100 [00:13<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Loss: 3.2341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 100/100 [00:13<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Loss: 2.8205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 100/100 [00:13<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Loss: 2.2626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 100/100 [00:13<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Loss: 2.3443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 100/100 [00:13<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Loss: 1.9071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 100/100 [00:13<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Loss: 1.7390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 100/100 [00:13<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Loss: 1.5758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|██████████| 100/100 [00:13<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Loss: 1.6657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|██████████| 100/100 [00:13<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21, Loss: 1.4523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|██████████| 100/100 [00:13<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22, Loss: 1.4952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|██████████| 100/100 [00:13<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23, Loss: 1.3787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|██████████| 100/100 [00:13<00:00,  7.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24, Loss: 1.6149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|██████████| 100/100 [00:13<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25, Loss: 1.5258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|██████████| 100/100 [00:13<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26, Loss: 1.3184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|██████████| 100/100 [00:13<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27, Loss: 1.0273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|██████████| 100/100 [00:13<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28, Loss: 0.9966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|██████████| 100/100 [00:13<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29, Loss: 0.8788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 100%|██████████| 100/100 [00:13<00:00,  7.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30, Loss: 0.9298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "total_correct = 0\n",
        "total_count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_dataloader, desc='Validation'):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions = logits.argmax(dim=-1)\n",
        "\n",
        "        total_correct += (predictions == labels).sum().item()\n",
        "        total_count += labels.numel()\n",
        "\n",
        "accuracy = total_correct / total_count\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_we9H3j_XuNT",
        "outputId": "9e0a790f-776c-4afb-f97b-0e0d64adb200"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 26/26 [00:01<00:00, 20.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Specify the directory where you want to save the model in Google Drive\n",
        "output_dir = \"/content/drive/MyDrive/fine_tuned_bert_model\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "0\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(\"Model saved successfully at:\", output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtDGh8yJFNlj",
        "outputId": "2f442a5d-7a9b-4b11-f505-45e6cbeb8339"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully at: /content/drive/MyDrive/fine_tuned_bert_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the test input tensor to the same device as the model\n",
        "test_sentence = \"You am not subscribed\"\n",
        "tokenized_test_sentence = tokenizer(test_sentence, return_tensors='pt', padding=True, truncation=True)\n",
        "tokenized_test_sentence = {key: value.to(device) for key, value in tokenized_test_sentence.items()}\n",
        "\n",
        "# Perform inference\n",
        "outputs = model(**tokenized_test_sentence)\n",
        "predicted_ids = torch.argmax(outputs.logits[0], dim=-1)\n",
        "predicted_sentence = tokenizer.decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "# Capitalize the first letter of the predicted sentence\n",
        "predicted_sentence = predicted_sentence.capitalize()\n",
        "\n",
        "print(\"Corrected Sentence:\", predicted_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADwJdZmp9gJ_",
        "outputId": "276f5bfd-71c8-4b5e-b809-2f5013f7df47"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Sentence: You are not subscribed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# Load the fine-tuned BERT model and tokenizer\n",
        "model_path = \"./fine_tuned_bert_model\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "model = BertForMaskedLM.from_pretrained(model_path)\n",
        "\n",
        "# Function to correct grammar using the BERT model\n",
        "def correct_grammar(sentence):\n",
        "    tokenized_sentence = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
        "    outputs = model(**tokenized_sentence)\n",
        "    predicted_ids = torch.argmax(outputs.logits[0], dim=-1)\n",
        "    corrected_sentence = tokenizer.decode(predicted_ids, skip_special_tokens=True)\n",
        "    return corrected_sentence.capitalize()\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"Grammar Correction App\")\n",
        "\n",
        "    # Input text box for user input\n",
        "    input_text = st.text_area(\"Enter a sentence with grammatical errors:\", \"\")\n",
        "\n",
        "    # Button to trigger grammar correction\n",
        "    if st.button(\"Correct Grammar\"):\n",
        "        if input_text.strip() == \"\":\n",
        "            st.warning(\"Please enter a sentence.\")\n",
        "        else:\n",
        "            corrected_text = correct_grammar(input_text)\n",
        "            st.success(\"Corrected Sentence:\")\n",
        "            st.write(corrected_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "4W60_L34GO_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import streamlit as st\n",
        "\n",
        "# Load the fine-tuned model and tokenizer from Google Drive\n",
        "output_dir = \"/content/drive/MyDrive/fine_tuned_bert_model\"\n",
        "model = BertForMaskedLM.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define function for inference\n",
        "def correct_sentence(input_sentence):\n",
        "    # Tokenize input sentence\n",
        "    tokenized_input = tokenizer(input_sentence, return_tensors='pt', padding=True, truncation=True)\n",
        "    tokenized_input = {key: value.to(device) for key, value in tokenized_input.items()}\n",
        "\n",
        "    # Perform inference\n",
        "    outputs = model(**tokenized_input)\n",
        "    predicted_ids = torch.argmax(outputs.logits[0], dim=-1)\n",
        "    predicted_sentence = tokenizer.decode(predicted_ids, skip_special_tokens=True)\n",
        "    predicted_sentence = predicted_sentence.capitalize()\n",
        "\n",
        "    return predicted_sentence\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Grammar Correction with BERT\")\n",
        "\n",
        "# Input text area for user to enter a sentence\n",
        "user_input = st.text_area(\"Enter a sentence to correct\")\n",
        "\n",
        "# Button to perform correction\n",
        "if st.button(\"Correct\"):\n",
        "    if user_input.strip():\n",
        "        corrected_sentence = correct_sentence(user_input)\n",
        "        st.write(\"Corrected Sentence:\", corrected_sentence)\n",
        "    else:\n",
        "        st.write(\"Please enter a sentence for correction\")\n"
      ],
      "metadata": {
        "id": "KZMEnyZzYaFz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}